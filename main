#import llibraries
from collections import deque
import numpy as np

import pandas as pd
import random

from sklearn.preprocessing import StandardScaler,MinMaxScaler

import time
import random as rn
rn.seed(137)
from numpy.random import seed
seed(1)
from sklearn.metrics import mean_squared_error , r2_score,mean_absolute_error


#Model Library
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.python.keras.layers import Dense,Input, Dropout,LSTM,Bidirectional,GRU,RNN
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint

from tensorflow.keras.models import load_model, Model



#predicting Daily ET0 and swvl: the model input are daily 'Temp_Min','Temp_Max','HRel_Avg','RadSol_Avg','VelVento_Avg','ET0',swl1,swvl2,swvl3,swvl4
# at time t-n,.., t-1 and the output is ET0,swl1,swvl2,swvl3,swvl4 at time t

#hyperparameters:
seq_len=8 #(number of days for predicting ET)
Batch_size=124

Epoch=500
location_to_predict='Fadagosa'
lr=1e-4
decay=1e-5
hidden_units=512# #number of hidden neurons in LSTM layer 32,64,128,256,
dropout_size=0.2
n_out=4 #number of output 
NAME = f"ET0_SWVL_diario_{location_to_predict}_{Batch_size}_{hidden_units}-{dropout_size}-{int(time.time())}.h5" 

#opening the dataset as a Datafram(fadagosa)  
dataset='fadagosa_mean_ET0.csv'
df0=pd.read_csv(dataset)
df0.dropna(inplace=True) #removing rows wih Nan


#set time step as index
df0['TIME_STEP']=pd.date_range(start='2010-01-7 00:00:00', periods=len(df0), freq='D')
df0.set_index('TIME_STEP', inplace=True)  # set time as index so we can join them on this shared
#df.index.name
df0.drop(['TIMESTAMP'],axis=1,inplace=True)
df0.dropna(inplace=True)


#removing TIMSTEP , ET0_Avg columns in the dataset 
#df.drop(['TIMESTAMP'],axis=1,inplace=True)
df0.dropna(inplace=True)
df0.drop(['Humidity_max','Tem_Avg','Humidity_min','wind_speed_max'],axis=1, inplace=True)

#Atalaia dataset
dataset1='D:/A/Data/new_data/Atalaia_5_days.csv'
df1=pd.read_csv(dataset1)
df1.dropna(inplace=True) #removing rows wih Nan


#set time step as index
df1['TIME_STEP']=pd.date_range(start='2013-08-17 00:00:00', periods=len(df1), freq='D')
df1.set_index('TIME_STEP', inplace=True)  # set time as index so we can join them on this shared
#df.index.name
df1.dropna(inplace=True)
df1.drop(['Humidity_max','Tem_Avg','Humidity_min','VelVento_Max'],axis=1, inplace=True)

#set Borralheira dataset
dataset='D:/A/Data/new_data/Borralheira_7_data.csv'
df2=pd.read_csv(dataset)
df2.dropna(inplace=True) #removing rows wih Nan


#set time step as index
df2['TIME_STEP']=pd.date_range(start='2013-08-19 00:00:00', periods=len(df2), freq='D')
df2.set_index('TIME_STEP', inplace=True)  # set time as index so we can join them on this shared
#df.index.name

df2.dropna(inplace=True)
df2.drop([ 'Humidity_max','Tem_Avg','Humidity_min', 'VelVento_Max','runoff'],axis=1, inplace=True)
#creating target.

#data preprocessing function
def preprocess_df(df):
    #removing outliers for the data
    #z = np.abs(stats.zscore(df))
    #df= df[(z < 3).all(axis=1)]
    
    #normilized the data
    values=df.values
    scaler=StandardScaler()
    values_normal=scaler.fit_transform(values)
  
    df=pd.DataFrame(values_normal, columns= df.columns, index=df.index)
    

    return df,scaler   

#split the dataset to the input and output of the model
def data_split(df,train=True):    
    sequential_data=[]
    prev_day = deque(maxlen=seq_len)
    
    for i in df.values:
        prev_day.append([n for n in i[:-n_out]])
        
        if len(prev_day)==seq_len:
            sequential_data.append([np.array(prev_day),i[-n_out:]])#i[-1] is target wich is ET 
            #prev_day.clear() #maybe we need this to add new days to the list
     
            
    if train:
       random.shuffle(sequential_data)
        
    X=[]             
    Y=[]
    
    for seq , target in sequential_data:
        X.append(seq)
        Y.append(target)
    return np.array(X), np.array(Y)   



#R2_score metric
    
def r_square(y_true, y_pred):
    y_true= tf.convert_to_tensor(y_true, np.float32)
    SS_res = tf.keras.backend.sum(tf.keras.backend.square( y_true-y_pred ))
    SS_tot = tf.keras.backend.sum(tf.keras.backend.square( y_true - tf.keras.backend.mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + tf.keras.backend.epsilon()) )
    
#RMSR metric    
def rmse(y_true, y_pred):
    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true), axis=-1))

def creat_Lstm():
        
         
    input1=Input(shape=(X_train.shape[1],X_train.shape[2]))
    BLSTM=(Bidirectional(tf.keras.layers.LSTM(hidden_units,recurrent_dropout=dropout_size, 
                                      return_state=True,activation='tanh',name='lstm1')))(input1)
    
    BLSTM=(Bidirectional(tf.keras.layers.LSTM(hidden_units,recurrent_dropout=dropout_size, 
                                      return_state=False,activation='tanh',name='lstm1')))(input1)
    

    output = Dense(n_out)(BLSTM)
    
    model = Model(inputs=input1, outputs=output)
    opt=Adam(learning_rate=lr,decay=decay)
    model.compile(optimizer=opt, loss='mse', metrics=['mse',"mae","RootMeanSquaredError", r_square] )
    
    return model



#separate the last 10 percent of the datasets as test data
times1=sorted(df0.index.values)
last_10per=int(0.1*len(times1))
train_df0=df0.iloc[:len(df0)-last_10per,:]
test_df0=df0.iloc[len(df0)-last_10per:,:]

times2=sorted(df1.index.values)
last_10per=int(0.1*len(times2))


train_df1=df1.iloc[:len(df1)-last_10per,:]
test_df1=df1.iloc[len(df1)-last_10per:,:]

times3=sorted(df2.index.values)
last_10per=int(0.1*len(times3))


train_df2=df2.iloc[:len(df2)-last_10per,:]
test_df2=df2.iloc[len(df2)-last_10per:,:]

#concatenate all training set together
train_df=pd.concat([train_df1,train_df2,train_df0],axis=0)
test_df=pd.concat([test_df0,test_df1],axis=0)
#preprossecing the train data
train_df, scaler=preprocess_df(train_df)

#splite data for supervised learning
X_train,Y_train=data_split(train_df)

model=creat_Lstm()

#define the early stopping callback
Early=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=20,
    verbose=1,
    mode="min",
    baseline=None,
    restore_best_weights=True,
)

#training the model
history=model.fit(X_train,Y_train,batch_size=Batch_size,
                         epochs=Epoch, 
                         validation_split=0.2,
                         verbose=2,
                         callbacks=[Early]) 

import matplotlib.pyplot as plt

plt.plot(history.history['val_r_square'])
plt.plot(history.history["r_square"])
plt.title(r'$R^2$  during training (Loc. 1)')
plt.ylabel('$R^2$')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()
           
# plot training curve for rmse
plt.plot(history.history['RootMeanSquaredError'])
plt.plot(history.history['val_RootMeanSquaredError'])
plt.title('RMSE during training (Loc. 1)')
plt.ylabel('RSME')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='best')
plt.show()



#make a prediction for the fadagosa  

#preprocess the test set using scaler from the train set
test_df=test_df0
values=test_df.values
values=scaler.transform(values)
test_df3=pd.DataFrame(values, columns= test_df.columns, index=test_df.index)
 
X_test,Y_test=data_split(test_df3,train=False)

yhat = model.predict(X_test)



# invert scaling for actual
X_test_reshape = X_test.reshape((X_test.shape[0], X_test.shape[1]*X_test.shape[2]))
inv_y = np.concatenate((X_test_reshape[:, -(len(df0.columns)-n_out):],Y_test), axis=1)
inv_y = scaler.inverse_transform(inv_y)
inv_y=inv_y[:,-n_out:]

#invert scaling for prediction

inv_yhat = np.concatenate((X_test_reshape[:, -(len(df0.columns)-n_out):],yhat), axis=1)
inv_yhat = scaler.inverse_transform(inv_yhat)
inv_yhat=inv_yhat[:,-n_out:]


#calculate the metric
l=['ET0','swc1','swc2','swc3']

for i in range(n_out):
    mse =mean_squared_error(inv_y[:,i], inv_yhat[:,i])
    r2=r2_score(inv_y[:,i], inv_yhat[:,i])
    mae=mean_absolute_error(inv_y[:,i], inv_yhat[:,i])
    rmse=np.sqrt(mse)
    print(f' mse_{l[i]} : {mse}')
    print(f' rmse_{l[i]} : {rmse}')
    print(f'mae_{l[i]} : {mae}')
    print(f' r2_{l[i]} : {r2}')

mse=mean_squared_error(inv_y, inv_yhat) 
rmse=np.sqrt(mean_squared_error(inv_y, inv_yhat)    )
mae=mean_absolute_error(inv_y, inv_yhat)
r2=r_square(inv_y, inv_yhat)

print(f' MSE_general: {mse}')
print(f' RMSE_general: {rmse}')
print(f'r2_general: {r2}')
print(f'mae_general: {mae}')

import matplotlib.pyplot as plt#

#plot the true value vs. Prediction
for i in range(n_out):
    plt.scatter(inv_y[:,i],inv_yhat[:,i])
    plt.plot(inv_y[:,i], inv_y[:,i],'r', linestyle='dashed')
    plt.xlabel('True Values  ')
    plt.ylabel('Predictions ')
    plt.title (f'Predictions  v.s. true values of {l[i]} (Loc 2)')
    plt.show() 
    



